<!doctype html>
<html lang="en">
 <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>REINFORCEjs: Gridworld with Dynamic Programming</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- jquery and jqueryui -->
  <script src="external/jquery-2.1.3.min.js"></script>
  <link href="external/jquery-ui.min.css" rel="stylesheet">
  <script src="external/jquery-ui.min.js"></script>

  <!-- bootstrap -->
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
  <link href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css" rel="stylesheet">

  <!-- d3js -->
  <script type="text/javascript" src="external/d3.min.js"></script>

  <!-- markdown -->
  <script type="text/javascript" src="external/marked.js"></script>
  <script type="text/javascript" src="external/highlight.pack.js"></script>
  <link rel="stylesheet" href="external/highlight_default.css">
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- mathjax : nvm now loading dynamically
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
   -->

  <!-- rljs -->
  <script type="text/javascript" src="lib/rl.js"></script>

  <!-- flotjs -->
  <script src="external/jquery.flot.min.js"></script>

  <!-- GA -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-3698471-24', 'auto');
  ga('send', 'pageview');
  </script>

  <style>
  #wrap {
    width:800px;
    margin-left: auto;
    margin-right: auto;
  }
  body {
    font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;
  }
  #draw {
    margin-left: 100px;
  }
  #exp {
    margin-top: 20px;
    font-size: 16px;
  }
  h2 {
    text-align: center;
    font-size: 30px;
  }
  svg {
    cursor: pointer;
  }
  </style>

  <script type="application/javascript">

    // Gridworld
    var Gridworld = function(){
      this.Rarr = null; // reward array
      this.T = null; // cell types, 0 = normal, 1 = cliff
      this.reset()
    }
    Gridworld.prototype = {
      reset: function() {

        // hardcoding one gridworld for now
        this.gh = 10;
        this.gw = 10;
        this.gs = this.gh * this.gw; // number of states

        // specify some rewards
        var Rarr = R.zeros(this.gs);
        var T = R.zeros(this.gs);
        Rarr[55] = 1;

        Rarr[54] = -1;
        //Rarr[63] = -1;
        Rarr[64] = -1;
        Rarr[65] = -1;
        Rarr[85] = -1;
        Rarr[86] = -1;

        Rarr[37] = -1;
        Rarr[33] = -1;
        //Rarr[77] = -1;
        Rarr[67] = -1;
        Rarr[57] = -1;

        // make some cliffs
        for(q=0;q<8;q++) { var off = (q+1)*this.gh+2; T[off] = 1; Rarr[off] = 0; }
        for(q=0;q<6;q++) { var off = 4*this.gh+q+2; T[off] = 1; Rarr[off] = 0; }
        T[5*this.gh+2] = 0; Rarr[5*this.gh+2] = 0; // make a hole
        this.Rarr = Rarr;
        this.T = T;
      },
      reward: function(s,a,ns) {
        // reward of being in s, taking action a, and ending up in ns
        return this.Rarr[s];
      },
      nextStateDistribution: function(s,a) {
        // given (s,a) return distribution over s' (in sparse form)
        if(this.T[s] === 1) {
          // cliff! oh no!
          // var ns = 0; // reset to state zero (start)
        } else if(s === 55) {
          // agent wins! teleport to start
          var ns = this.startState();
          while(this.T[ns] === 1) {
            var ns = this.randomState();
          }
        } else {
          // ordinary space
          var nx, ny;
          var x = this.stox(s);
          var y = this.stoy(s);
          if(a === 0) {nx=x-1; ny=y;}
          if(a === 1) {nx=x; ny=y-1;}
          if(a === 2) {nx=x; ny=y+1;}
          if(a === 3) {nx=x+1; ny=y;}
          var ns = nx*this.gh+ny;
          if(this.T[ns] === 1) {
            // actually never mind, this is a wall. reset the agent
            var ns = s;
          }
        }
        // gridworld is deterministic, so return only a single next state
        return ns;
      },
      sampleNextState: function(s,a) {
        // gridworld is deterministic, so this is easy
        var ns = this.nextStateDistribution(s,a);
        var r = this.Rarr[s]; // observe the raw reward of being in s, taking a, and ending up in ns
        r -= 0.01; // every step takes a bit of negative reward
        var out = {'ns':ns, 'r':r};
        if(s === 55 && ns === 0) {
          // episode is over
          out.reset_episode = true;
        }
        return out;
      },
      allowedActions: function(s) {
        var x = this.stox(s);
        var y = this.stoy(s);
        var as = [];
        if(x > 0) { as.push(0); }
        if(y > 0) { as.push(1); }
        if(y < this.gh-1) { as.push(2); }
        if(x < this.gw-1) { as.push(3); }
        return as;
      },
      randomState: function() { return Math.floor(Math.random()*this.gs); },
      startState: function() { return 0; },
      getNumStates: function() { return this.gs; },
      getMaxNumActions: function() { return 4; },

      // private functions
      stox: function(s) { return Math.floor(s/this.gh); },
      stoy: function(s) { return s % this.gh; },
      xytos: function(x,y) { return x*this.gh + y; },
    }

    // ------
    // UI
    // ------
    var rs = {};
    var trs = {};
    var tvs = {};
    var pas = {};
    var cs = 60;  // cell size
    var initGrid = function() {
      var d3elt = d3.select('#draw');
      d3elt.html('');
      rs = {};
      trs = {};
      tvs = {};
      pas = {};

      var gh= env.gh; // height in cells
      var gw = env.gw; // width in cells
      var gs = env.gs; // total number of cells

      var w = 600;
      var h = 600;
      svg = d3elt.append('svg').attr('width', w).attr('height', h)
        .append('g').attr('transform', 'scale(1)');

      // define a marker for drawing arrowheads
      svg.append("defs").append("marker")
        .attr("id", "arrowhead")
        .attr("refX", 3)
        .attr("refY", 2)
        .attr("markerWidth", 3)
        .attr("markerHeight", 4)
        .attr("orient", "auto")
        .append("path")
          .attr("d", "M 0,0 V 4 L3,2 Z");

      for(var y=0;y<gh;y++) {
        for(var x=0;x<gw;x++) {
          var xcoord = x*cs;
          var ycoord = y*cs;
          var s = env.xytos(x,y);

          var g = svg.append('g');
          // click callbackfor group
          g.on('click', function(ss) {
            return function() { cellClicked(ss); } // close over s
          }(s));

          // set up cell rectangles
          var r = g.append('rect')
            .attr('x', xcoord)
            .attr('y', ycoord)
            .attr('height', cs)
            .attr('width', cs)
            .attr('fill', '#FFF')
            .attr('stroke', 'black')
            .attr('stroke-width', 2);
          rs[s] = r;

          // reward text
          var tr = g.append('text')
            .attr('x', xcoord + 5)
            .attr('y', ycoord + 55)
            .attr('font-size', 10)
            .text('');
          trs[s] = tr;

          // skip rest for cliffs
          if(env.T[s] === 1) { continue; }

          // value text
          var tv = g.append('text')
            .attr('x', xcoord + 5)
            .attr('y', ycoord + 20)
            .text('');
          tvs[s] = tv;

          // policy arrows
          pas[s] = []
          for(var a=0;a<4;a++) {
            var pa = g.append('line')
              .attr('x1', xcoord)
              .attr('y1', ycoord)
              .attr('x2', xcoord)
              .attr('y2', ycoord)
              .attr('stroke', 'black')
              .attr('stroke-width', '2')
              .attr("marker-end", "url(#arrowhead)");
            pas[s].push(pa);
          }
        }
      }

      // append agent position circle
      svg.append('circle')
        .attr('cx', -100)
        .attr('cy', -100)
        .attr('r', 15)
        .attr('fill', '#FF0')
        .attr('stroke', '#000')
        .attr('id', 'cpos');

    }

    var drawGrid = function() {
      var gh= env.gh; // height in cells
      var gw = env.gw; // width in cells
      var gs = env.gs; // total number of cells

      var sx = env.stox(state);
      var sy = env.stoy(state);
      d3.select('#cpos')
        .attr('cx', sx*cs+cs/2)
        .attr('cy', sy*cs+cs/2);

      // updates the grid with current state of world/agent
      for(var y=0;y<gh;y++) {
        for(var x=0;x<gw;x++) {
          var xcoord = x*cs;
          var ycoord = y*cs;
          var r=255,g=255,b=255;
          var s = env.xytos(x,y);

          // get value of state s under agent policy
          if(typeof agent.V !== 'undefined') {
            var vv = agent.V[s];
          } else if(typeof agent.Q !== 'undefined'){
            var poss = env.allowedActions(s);
            var vv = -1;
            for(var i=0,n=poss.length;i<n;i++) {
              var qsa = agent.Q[poss[i]*gs+s];
              if(i === 0 || qsa > vv) { vv = qsa; }
            }
          }

          // var poss = env.allowedActions(s);
          // var vv = -1;
          // for(var i=0,n=poss.length;i<n;i++) {
          //   var qsa = agent.e[poss[i]*gs+s];
          //   if(i === 0 || qsa > vv) { vv = qsa; }
          // }

          var ms = 100;
          if(vv > 0) { g = 255; r = 255 - vv*ms; b = 255 - vv*ms; }
          if(vv < 0) { g = 255 + vv*ms; r = 255; b = 255 + vv*ms; }
          var vcol = 'rgb('+Math.floor(r)+','+Math.floor(g)+','+Math.floor(b)+')';
          if(env.T[s] === 1) { vcol = "#AAA"; rcol = "#AAA"; }

          // update colors of rectangles based on value
          var r = rs[s];
          if(s === selected) {
            // highlight selected cell
            r.attr('fill', '#FF0');
          } else {
            r.attr('fill', vcol);
          }

          // write reward texts
          var rv = env.Rarr[s];
          var tr = trs[s];
          if(rv !== 0) {
            tr.text('R ' + rv.toFixed(1))
          }

          // skip rest for cliff
          if(env.T[s] === 1) continue;

          // write value
          var tv = tvs[s];
          tv.text(vv.toFixed(2));

          // update policy arrows
          var paa = pas[s];
          for(var a=0;a<4;a++) {
            var pa = paa[a];
            var prob = agent.P[a*gs+s];
            if(prob < 0.01) { pa.attr('visibility', 'hidden'); }
            else { pa.attr('visibility', 'visible'); }
            var ss = cs/2 * prob * 0.9;
            if(a === 0) {nx=-ss; ny=0;}
            if(a === 1) {nx=0; ny=-ss;}
            if(a === 2) {nx=0; ny=ss;}
            if(a === 3) {nx=ss; ny=0;}
            pa.attr('x1', xcoord+cs/2)
              .attr('y1', ycoord+cs/2)
              .attr('x2', xcoord+cs/2+nx)
              .attr('y2', ycoord+cs/2+ny);
          }
        }
      }
    }

    var selected = -1;
    var cellClicked = function(s) {
      if(s === selected) {
        selected = -1; // toggle off
        $("#creward").html('(select a cell)');
      } else {
        selected = s;
        $("#creward").html(env.Rarr[s].toFixed(2));
        $("#rewardslider").slider('value', env.Rarr[s]);
      }
      drawGrid(); // redraw
    }

    var goslow = function() {
      steps_per_tick = 1;
    }
    var gonormal = function(){
      steps_per_tick = 10;
    }
    var gofast = function() {
      steps_per_tick = 25;
    }
    var steps_per_tick = 1;
    var sid = -1;
    var nsteps_history = [];
    var nsteps_counter = 0;
    var nflot = 1000;
    var tdlearn = function() {
      if(sid === -1) {
        sid = setInterval(function(){
          for(var k=0;k<steps_per_tick;k++) {

            var a = agent.act(state); // ask agent for an action
            var obs = env.sampleNextState(state, a); // run it through environment dynamics
            agent.learn(obs.r); // allow opportunity for the agent to learn
            state = obs.ns; // evolve environment to next state
            nsteps_counter += 1;
            if(typeof obs.reset_episode !== 'undefined') {
              agent.resetEpisode();
              // record the reward achieved
              if(nsteps_history.length >= nflot) {
                nsteps_history = nsteps_history.slice(1);
              }
              nsteps_history.push(nsteps_counter);
              nsteps_counter = 0;
            }
          }
          // keep track of reward history
          drawGrid(); // draw
        }, 20);
      } else {
        clearInterval(sid);
        sid = -1;
      }
    }

    function resetAgent() {
      eval($("#agentspec").val())
      agent = new RL.TDAgent(env, spec);
      $("#slider").slider('value', agent.epsilon);
      $("#eps").html(agent.epsilon.toFixed(2));
      state = env.startState(); // move state to beginning too
      drawGrid();
    }

    function resetAll() {
      env.reset();
      agent.reset();
      drawGrid();
    }

    function initGraph() {
      var container = $("#flotreward");
      var res = getFlotRewards();
      series = [{
        data: res,
        lines: {fill: true}
      }];
      var plot = $.plot(container, series, {
        grid: {
          borderWidth: 1,
          minBorderMargin: 20,
          labelMargin: 10,
          backgroundColor: {
            colors: ["#FFF", "#e4f4f4"]
          },
          margin: {
            top: 10,
            bottom: 10,
            left: 10,
          }
        },
        xaxis: {
          min: 0,
          max: nflot
        },
        yaxis: {
          min: 0,
          max: 1000
        }
      });

      setInterval(function(){
        series[0].data = getFlotRewards();
        plot.setData(series);
        plot.draw();
      }, 100);
    }
    function getFlotRewards() {
      // zip rewards into flot data
      var res = [];
      for(var i=0,n=nsteps_history.length;i<n;i++) {
        res.push([i, nsteps_history[i]]);
      }
      return res;
    }

    var state;
    var agent, env;
    function start() {
      env = new Gridworld(); // create environment
      state = env.startState();
      eval($("#agentspec").val())
      agent = new RL.TDAgent(env, spec);
      //agent = new RL.ActorCriticAgent(env, {'gamma':0.9, 'epsilon':0.2});

      // slider sets agent epsilon
      $( "#slider" ).slider({
        min: 0,
        max: 1,
        value: agent.epsilon,
        step: 0.01,
        slide: function(event, ui) {
          agent.epsilon = ui.value;
          $("#eps").html(ui.value.toFixed(2));
        }
      });

      $("#rewardslider").slider({
        min: -5,
        max: 5.1,
        value: 0,
        step: 0.1,
        slide: function(event, ui) {
          if(selected >= 0) {
            env.Rarr[selected] = ui.value;
            $("#creward").html(ui.value.toFixed(2));
            drawGrid();
          } else {
            $("#creward").html('(select a cell)');
          }
        }
      });

      $("#eps").html(agent.epsilon.toFixed(2));
      $("#slider").slider('value', agent.epsilon);

      // render markdown
      $(".md").each(function(){
        $(this).html(marked($(this).html()));
      });
      renderJax();

      initGrid();
      drawGrid();
      initGraph();
    }

    var jaxrendered = false;
    function renderJax() {
      if(jaxrendered) { return; }
      (function () {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src  = "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
        document.getElementsByTagName("head")[0].appendChild(script);
        jaxrendered = true;
      })();
    }

  </script>

 </head>
<body onload="start();">

  <a href="https://github.com/karpathy/reinforcejs"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

  <div id="wrap">


    <div id="mynav" style="border-bottom:1px solid #999; padding-bottom: 10px;display: none;margin-bottom:50px;">
      <div>
        <img src="loop.svg" style="width:50px;height:50px;float:left;">
        <h1 style="font-size:50px;">REINFORCE<span style="color:#058;">js</span></h1>
      </div>
      <ul class="nav nav-pills">
        <li role="presentation"><a href="index.html">About</a></li>
        <li role="presentation"><a href="gridworld_dp.html">GridWorld: DP</a></li>
        <li role="presentation" class="active"><a href="gridworld_td.html">GridWorld: TD</a></li>
        <li role="presentation"><a href="puckworld.html">PuckWorld: DQN</a></li>
        <li role="presentation"><a href="waterworld.html">WaterWorld: DQN</a></li>
      </ul>
     </div>

   <!-- <h2>Temporal Difference Learning Gridworld Demo</h2> -->
   <br>

<textarea id="agentspec" style="width:100%;height:270px;">
// agent parameter spec to play with (this gets eval()'d on Agent reset)
var spec = {}
spec.update = 'qlearn'; // 'qlearn' or 'sarsa'
spec.gamma = 0.9; // discount factor, [0, 1)
spec.epsilon = 0.2; // initial epsilon for epsilon-greedy policy, [0, 1)
spec.alpha = 0.1; // value function learning rate
spec.lambda = 0; // eligibility trace decay, [0,1). 0 = no eligibility traces
spec.replacing_traces = true; // use replacing or accumulating traces
spec.planN = 50; // number of planning steps per iteration. 0 = no planning

spec.smooth_policy_update = true; // non-standard, updates policy smoothly to follow max_a Q
spec.beta = 0.1; // learning rate for smooth policy update
</textarea>
   <button class="btn btn-danger" onclick="resetAgent()" style="width:150px;height:50px;margin-bottom:5px;">Reinit agent</button>
   <button class="btn btn-primary" onclick="tdlearn()" style="width:170px;height:50px;margin-bottom:5px;">Toggle TD Learning</button>
   <button class="btn btn-success" onclick="gofast()" style="width:150px;height:50px;margin-bottom:5px;">Go fast</button>
   <button class="btn btn-success" onclick="gonormal()" style="width:150px;height:50px;margin-bottom:5px;">Go normal</button>
   <button class="btn btn-success" onclick="goslow()" style="width:150px;height:50px;margin-bottom:5px;">Go slow</button>


   <br>
   Exploration epsilon: <span id="eps">0.15</span> <div id="slider"></div>

   <br><br>

   <div id="draw"></div>

   <div id="rewardui">
   Cell reward: <span id="creward">(select a cell)</span> <div id="rewardslider"></div>
   </div>

   <div style="display:none;">
      Number of actions before reaching the goal state (low is good):
     <div id="flotreward" style="width:800px; height: 400px;"></div>
   </div>

   <div id="exp" class="md">


<pre><code class="js">
// create environment
env = new Gridworld();
// create the agent, yay!
var spec = { alpha: 0.01 } // see full options on top of this page
agent = new RL.TDAgent(env, spec);

setInterval(function(){ // start the learning loop
  var action = agent.act(s); // s is an integer, action is integer
  // execute action in environment and get the reward
  agent.learn(reward); // the agent improves its Q,policy,model, etc.
}, 0);
</code></pre>


   </div>
   </div>
   <br><br><br><br><br>
 </body>
</html>
